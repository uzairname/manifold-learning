{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/home/moham147/experiments/manifold-learning/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clock dataset saved in 'data/clock_images', resized to 64x64\n",
      "Labels saved in 'data/clock_labels.csv'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Create output directory\n",
    "IMG_DIR = \"data/clock_images\"\n",
    "os.makedirs(IMG_DIR, exist_ok=True)\n",
    "\n",
    "# Define the fixed output size\n",
    "IMG_SIZE = 64\n",
    "\n",
    "LABELS_FILE = \"data/clock_labels.csv\"\n",
    "\n",
    "def draw_clock(hour, minute, save_path, img_size=IMG_SIZE):\n",
    "    \"\"\"Generates a clock image at a given hour/minute and resizes it.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(2,2), dpi=100)  # Higher DPI for better quality\n",
    "\n",
    "    ax.set_xlim(-1, 1)\n",
    "    ax.set_ylim(-1, 1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_frame_on(False)\n",
    "\n",
    "    # Draw clock face\n",
    "    circle = plt.Circle((0, 0), 1, edgecolor=\"black\", facecolor=\"white\", lw=3)\n",
    "    ax.add_patch(circle)\n",
    "\n",
    "    # Compute hand angles\n",
    "    hour_angle = np.pi / 2 - (2 * np.pi * (hour % 12) / 12) - (2 * np.pi * (minute / 60) / 12)\n",
    "    minute_angle = np.pi / 2 - (2 * np.pi * minute / 60)\n",
    "\n",
    "    # Draw hands\n",
    "    ax.plot([0, 0.5 * np.cos(hour_angle)], [0, 0.5 * np.sin(hour_angle)], 'k', lw=5)  # Hour hand\n",
    "    ax.plot([0, 0.8 * np.cos(minute_angle)], [0, 0.8 * np.sin(minute_angle)], 'k', lw=3)  # Minute hand\n",
    "\n",
    "    # Save image with padding\n",
    "    plt.axis('off')\n",
    "    temp_path = \"data/temp.png\"\n",
    "    plt.savefig(temp_path, bbox_inches='tight', pad_inches=0.1)  # Small padding\n",
    "    plt.close()\n",
    "\n",
    "    # Load image and resize\n",
    "    img = Image.open(temp_path).convert(\"L\")  # Convert to grayscale\n",
    "    img = img.resize((img_size, img_size), Image.LANCZOS)  # Resize to power of 2\n",
    "    img.save(save_path)\n",
    "\n",
    "# Open CSV file for writing labels\n",
    "with open(LABELS_FILE, mode='w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"filename\", \"hour\", \"minute\", \"time_in_minutes\"])\n",
    "\n",
    "    # Generate clock images at different times\n",
    "    for hour in range(12):  # 12-hour format\n",
    "        for minute in range(0, 60, 1):  # Every 5 minutes\n",
    "            filename = f\"clock_{hour:02d}_{minute:02d}.png\"\n",
    "            save_path = os.path.join(IMG_DIR, filename)\n",
    "            draw_clock(hour, minute, save_path)\n",
    "\n",
    "            # Compute single number label (time in minutes past midnight)\n",
    "            time_in_minutes = hour * 60 + minute\n",
    "\n",
    "            # normalize minute, hour, and total minutes\n",
    "            hour_label = hour / 12\n",
    "            minute_label = minute / 60\n",
    "            time_in_minutes_label = time_in_minutes / (12 * 60)\n",
    "\n",
    "            # Write to CSV\n",
    "            writer.writerow([filename, hour_label, minute_label, time_in_minutes_label])\n",
    "\n",
    "print(f\"Clock dataset saved in '{IMG_DIR}', resized to {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"Labels saved in '{LABELS_FILE}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCABAAEABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APf6CcDJrj7rx/bT3ktj4b0668QXkTbZDZ4W3ib0edvkB9hk1YtLnxvOu+503Q7QHon2yWVh9cRgflRd3PjeBd9tpuh3YHJT7ZLEx+mYyPzqva+P7aC8jsfEmnXXh+8lbbGbzDW8reiTr8hPscGuwByMiignAya4Am4+JN3NHHNLb+EIJDGzRMUfVHU4YBhyIQeOPvc9q7eysbXTbOKzsreK3tol2xxRIFVR7AVU8RMV8M6qykqws5iCDgj5DVTwQ7SeAvDruzM7aZbFmY5JPlLyTWte2NrqVnLZ3tvFcW0q7ZIpUDKw9wa4gG4+G13DHJNLceEJ5BGrSsXfS3Y4UFjyYSeOfu8dq78HIyK4/wAf3VxPaaf4asZWiu9duPspkQ4aK3A3TOPcIMD3YV1FjZW+nWMFlaQrDbQRrHFGo4VQMAVYrL8Sf8ivq3/XlN/6A1U/Av8AyT7w3/2C7b/0UtdBVe+srbUbGeyu4Vmtp42jljYcMpGCK5fwBdXEFpqHhq+laW70K4+yiRzlpbcjdC59yhwfdTVi4tBP8TbG5fkW2kzhAezPLGM/kpFdRRWV4mZU8K6uzEBVspiSegHltVLwBLHN8PPDjRSK6jTLdSVORkRgEfgQR+FdFRXL29oIPibfXKcC50mAOB3ZJZBn8mAouLsQfE2xtn4FzpM5QnuySxnH5MTXUUVh+M/+RG8Qf9g24/8ARbVjfCT/AJJZ4f8A+vc/+htXa0Vy9vdif4m31snIttJgLkdmeWQ4/JQar+P7W4gtNP8AEtjE0t3oVx9qMaDLS25G2ZB7lDke6iuosb231GxgvbSZZraeNZIpFPDKRkGrFYnjIFvA+vgAknTbjAA6/u2rH+Eysnwu0BWUqwtzkEYP32rs6r317badYz3t3MsNtBG0ksjHhVAyTXL+ALW4ntNQ8S30TRXeu3H2oRuMNFbgbYUPuEGT7sa7AjIwa4Ai4+G13NJHDLceEJ5DIyxKXfS3Y5YhRyYSeePu89q7eyvrXUrOK8sriK4tpV3RyxOGVh7EVYoqve31rptnLeXtxFb20S7pJZXCqo9ya4gC4+JN3DJJDLb+EIJBIqyqUfVHU5UlTyIQecH73Hau/AwMCigjIwa4+68AW0F5JfeG9RuvD95K26QWeGt5W9Xgb5CfcYNWLS28bwLsudS0O7A6P9jliY/XEhH5UXdt43nXZbalodoD1f7HLKw+mZAPzqva+ALae8jvvEmo3XiC8ibdGLzC28TeqQL8gPucmuwAwMCiv//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAAETklEQVR4AaWXMWwURxRA/8zF2JbiWMIFxsiJfLYjmhgrrau4cYHkKFGsiCiJIuFDaQhYsSUofPTQYSqUDiQKR6lBCNzkREoQBQhjo6SIsRwER2Fz7M58/p/dvZ29nbnl4jnpdnZm/ps/f2bn/y8Q9lc+8IlrnemRMvOavginBlrnwaGb4QIoQdNtPmw0lNYIQspSd/dnZQCNpXTmpJYHKCng6Y0/7gOMfTx0qBf2tv/95ynA5NcnxgB1HoHZohXizeMAE9XadtqzXatOABy/iah02mpqkH0PEWtfQP/SI27WYRg09oIwNEKPlvphuoZIQ+ySBYT48hT0XaqTcBCJRWN1GBCkfqkPTr1sIdgAHeKdYVj4DzGghXBReOPXECOtVYC4swDDd9BGowWg5a/AyBqJN9cZ4HfwNgawVohrI7CSMUQKIPkzMPvKEicU/vxhCjCIV7NwxiY0AaT/PCy22CjASo8NMN2LULFW0QQEeBqqqOLFGwuwBq0AHlGF09QTlwQQ4GVYssBRtwNAm4tLZIeEEAPI/vBlTt6lAR8PnAXai2iOCKBV/Ui5zqcwW1wa8Gmsl4/U4zMZAUI8CbWEaTHcABr4J5yMhxtAiPdgobmqYgANXYB7EcEAFE4NvG7ZAIPxaEBb8frgFJ1TKnzRKHm7ttynfXdO8uVbT6n7qrXbUnETQRROD+zS3ZEvPg1Q692BaaMCTavlk7uVXi2sGQqrQvdW7j6RdG8yAK7BPHQkDzR8Hq6TKANK8PuxUWOMwonTARJHj62SKAG0WH88h8YeaX9xTeHc43WhGQD3YUZ0uAJag5ghQQYANLrGzbN4WmuEhPGuBr0bQLm/sz1gjsD+cgJQI+D0T9F8vi4NI2w41kAfjoa6/iUccDWbtsPsPhmAgz4NBOzilgeAMMjKMQB6fWNK6q/GefBtcQ+LGYBHnpuRT1u7YgB77Ub4T8gbFmOAeN7pl8CSAp4zmwHSZyce16ZsGWEaUHr2fzSQ8IyDBYZ0b9al77h4FUBR3+xOAMF6ka3zHA3rQQSQMAm3sHMN8BYJSv7h+NFVkY998pNmWkpi9eg4XUNkAwXfPNigq6GjosXGgzk+owSQ8AP85vsafFQkke9Zuu21rjEcgx9dLitzrdPk515cFb5PxqmDEldfnIu0Zm/ic22kwafyJ4cGlmtr61wJMATfOgCtzpX8rNO9a1RffX4h9uOW48u5d/QFGJaQVXUEGDSJM8SheCbvdF0hDsdjK44gy5q2WSX5xXyQxQRHmNeUSismzPslNazZBe4mcCUXaKZySY1iM0+gSYbMh7qJWPKkaNkb6pIO+WA7EYyecbB9xQ6V7WidV8Hh/g4ZJBcx0nGl2HTnbLtwn6YpSDguFiQchoC1aTvlCYI05fmoOOUxhoiSruVs0rU84Um62qV9o5/Ead/fG/60Lw+gO85KPOk26DTx5Atkn6mvuYP2l3wbxPv9vQMd535yolj9dQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=64x64>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# Display the first image to verify\n",
    "sample_image_path = os.path.join(IMG_DIR, \"clock_06_00.png\")\n",
    "image = Image.open(sample_image_path)\n",
    "display(image)\n",
    "print(image.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/opt/software-current/2023.06/x86_64/amd/zen4/software/Python-bundle-PyPI/2023.06-GCCcore-12.3.0/lib/python3.11/site-packages', '/opt/software-current/2023.06/x86_64/amd/zen4/software/virtualenv/20.23.1-GCCcore-12.3.0/lib/python3.11/site-packages', '/opt/software-current/2023.06/x86_64/amd/zen4/software/cryptography/41.0.1-GCCcore-12.3.0/lib/python3.11/site-packages', '/opt/software-current/2023.06/x86_64/amd/zen4/software/cffi/1.15.1-GCCcore-12.3.0/lib/python3.11/site-packages', '/opt/software-current/2023.06/x86_64/amd/zen4/software/Python/3.11.3-GCCcore-12.3.0/easybuild/python', '/opt/software-current/2023.06/x86_64/amd/zen4/software/Python/3.11.3-GCCcore-12.3.0/lib/python311.zip', '/opt/software-current/2023.06/x86_64/amd/zen4/software/Python/3.11.3-GCCcore-12.3.0/lib/python3.11', '/opt/software-current/2023.06/x86_64/amd/zen4/software/Python/3.11.3-GCCcore-12.3.0/lib/python3.11/lib-dynload', '/mnt/home/moham147/experiments/manifold-learning/.venv/lib/python3.11/site-packages', '/tmp/tmpwnxcrdz7']\n",
      "['/mnt/home/moham147/experiments/manifold-learning/.venv/lib/python3.11/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "import site\n",
    "print(site.getsitepackages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Autoencoder for 1xnxn images\n",
    "class DeepAutoencoder(nn.Module):\n",
    "  def __init__(self, input_dim, n_hidden):\n",
    "    super(DeepAutoencoder, self).__init__()\n",
    "    \n",
    "    # Encoder: Convolutional layers for feature extraction\n",
    "    self.encoder_conv = nn.Sequential(\n",
    "      nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(2, 2),\n",
    "      nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(2, 2),\n",
    "      nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(2, 2),\n",
    "    )\n",
    "\n",
    "    self.encoder_fc = nn.Sequential(\n",
    "        nn.Linear(512 * 256, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, n_hidden)  # Predicting hour and minute\n",
    "    )\n",
    "\n",
    "    # Decoder: Fully connected layers for reconstruction\n",
    "    self.decoder_fc = nn.Sequential(\n",
    "        nn.Linear(n_hidden, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 512 * 256)\n",
    "    )\n",
    "\n",
    "    # Decoder: Convolutional layers for image reconstruction\n",
    "    self.decoder_conv = nn.Sequential(\n",
    "      nn.ConvTranspose2d(512, 256, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.Upsample(scale_factor=2),\n",
    "      nn.ConvTranspose2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.Upsample(scale_factor=2),\n",
    "      nn.ConvTranspose2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.Upsample(scale_factor=2),\n",
    "      nn.ConvTranspose2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.ConvTranspose2d(32, 1, kernel_size=3, stride=1, padding=1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.encoder_conv(x)\n",
    "    x = torch.flatten(x, start_dim=1)  # Flatten the output\n",
    "    latent = self.encoder_fc(x)  # Encode to latent space\n",
    "\n",
    "    x = self.decoder_fc(latent)  # Decode from latent space\n",
    "    x = x.view(-1, 512, 16, 16)  # Reshape to match decoder input\n",
    "    x = self.decoder_conv(x)  # Decode to image space\n",
    "    reconstructed = torch.sigmoid(x)  # Apply sigmoid to ensure pixel values are between 0 and 1\n",
    "    return reconstructed, latent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Predict (hour, minute) as 2D labels\n",
    "class ClockRegressor(nn.Module):\n",
    "    def __init__(self, out_dim=2):\n",
    "        super(ClockRegressor, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512 * 256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, out_dim)  # Predicting hour and minute\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc(x)\n",
    "        x = torch.sigmoid(x) # Ensure output is between 0 and 1\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "# Custom dataset class\n",
    "class ClockDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None, supervised=True):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.supervised = supervised\n",
    "        self.images = os.listdir(img_dir)\n",
    "        self.labels_df = pd.read_csv(\"data/clock_labels.csv\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.supervised:\n",
    "            label_2d, label_1d = self.get_label(img_name)\n",
    "            return image, label_2d, label_1d\n",
    "        return image\n",
    "    \n",
    "\n",
    "    def get_label(self, img_name):\n",
    "      row = self.labels_df[self.labels_df[\"filename\"] == img_name].iloc[0]\n",
    "\n",
    "      return (\n",
    "          torch.tensor([row[\"hour\"], row[\"minute\"]], dtype=torch.float32),  # 2D label\n",
    "          torch.tensor(row[\"time_in_minutes\"], dtype=torch.float32)  # 1D label\n",
    "      )\n",
    "\n",
    "\n",
    "\n",
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Load supervised dataset\n",
    "train_dataset = ClockDataset(img_dir=IMG_DIR, transform=transform, supervised=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Load unsupervised dataset for autoencoder\n",
    "unsupervised_dataset = ClockDataset(img_dir=IMG_DIR, transform=transform, supervised=False)\n",
    "unsupervised_loader = DataLoader(unsupervised_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of images shape: torch.Size([32, 1, 64, 64])\n",
      "Batch of 2D labels shape: torch.Size([32, 2])\n",
      "Batch of 1D labels shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for batch, labels_2d, labels_1d in train_loader:\n",
    "    print(f\"Batch of images shape: {batch.shape}\")\n",
    "    print(f\"Batch of 2D labels shape: {labels_2d.shape}\")\n",
    "    print(f\"Batch of 1D labels shape: {labels_1d.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in autoencoder: 70442243\n"
     ]
    }
   ],
   "source": [
    "ae = DeepAutoencoder(n_hidden=2)\n",
    "\n",
    "# count number of parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Number of parameters in autoencoder: {count_parameters(ae)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x32768 and 131072x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m unsupervised_loader:\n\u001b[32m     12\u001b[39m         \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m         reconstructed, _ = \u001b[43mae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Get reconstructed image, ignore latent\u001b[39;00m\n\u001b[32m     14\u001b[39m         loss = criterion(reconstructed, batch)\n\u001b[32m     15\u001b[39m         \u001b[38;5;28mprint\u001b[39m(loss.item())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/experiments/manifold-learning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/experiments/manifold-learning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mDeepAutoencoder.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     60\u001b[39m x = \u001b[38;5;28mself\u001b[39m.encoder_conv(x)\n\u001b[32m     61\u001b[39m x = torch.flatten(x, start_dim=\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# Flatten the output\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m latent = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder_fc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Encode to latent space\u001b[39;00m\n\u001b[32m     64\u001b[39m x = \u001b[38;5;28mself\u001b[39m.decoder_fc(latent)  \u001b[38;5;66;03m# Decode from latent space\u001b[39;00m\n\u001b[32m     65\u001b[39m x = x.view(-\u001b[32m1\u001b[39m, \u001b[32m512\u001b[39m, \u001b[32m16\u001b[39m, \u001b[32m16\u001b[39m)  \u001b[38;5;66;03m# Reshape to match decoder input\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/experiments/manifold-learning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/experiments/manifold-learning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/experiments/manifold-learning/.venv/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/experiments/manifold-learning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/experiments/manifold-learning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/experiments/manifold-learning/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (32x32768 and 131072x256)"
     ]
    }
   ],
   "source": [
    "\n",
    "HIDDEN_UNITS = 2\n",
    "LEARNING_RATE = 0.0002\n",
    "\n",
    "ae = DeepAutoencoder(n_hidden=HIDDEN_UNITS)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(ae.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training loop for autoencoder\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in unsupervised_loader:\n",
    "        # Forward pass\n",
    "        reconstructed, _ = ae(batch) # Get reconstructed image, ignore latent\n",
    "        loss = criterion(reconstructed, batch)\n",
    "        print(loss.item())\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09335006773471832\n",
      "0.08971729129552841\n",
      "0.07255273312330246\n",
      "0.10962341725826263\n",
      "0.07685185968875885\n",
      "0.07334550470113754\n",
      "0.08128383755683899\n",
      "0.0822688490152359\n",
      "0.08242283761501312\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 17\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/projects/experiments/.venv/lib/python3.13/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/experiments/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/experiments/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "LEARNING_RATE = 0.0002\n",
    "\n",
    "regressor = ClockRegressor(out_dim=2)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(regressor.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training loop for regressor\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for batch, labels_2d, labels_1d in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = regressor(batch)\n",
    "        loss = criterion(outputs, labels_2d)\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
