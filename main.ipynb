{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/uz/projects/experiments/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clock dataset saved in 'data/clock_images', resized to 128x128\n",
      "Labels saved in 'data/clock_labels.csv'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Create output directory\n",
    "IMG_DIR = \"data/clock_images\"\n",
    "os.makedirs(IMG_DIR, exist_ok=True)\n",
    "\n",
    "# Define the fixed output size\n",
    "IMG_SIZE = 128\n",
    "\n",
    "LABELS_FILE = \"data/clock_labels.csv\"\n",
    "\n",
    "def draw_clock(hour, minute, save_path, img_size=IMG_SIZE):\n",
    "    \"\"\"Generates a clock image at a given hour/minute and resizes it.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(2,2), dpi=100)  # Higher DPI for better quality\n",
    "\n",
    "    ax.set_xlim(-1, 1)\n",
    "    ax.set_ylim(-1, 1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_frame_on(False)\n",
    "\n",
    "    # Draw clock face\n",
    "    circle = plt.Circle((0, 0), 1, edgecolor=\"black\", facecolor=\"white\", lw=3)\n",
    "    ax.add_patch(circle)\n",
    "\n",
    "    # Compute hand angles\n",
    "    hour_angle = np.pi / 2 - (2 * np.pi * (hour % 12) / 12) - (2 * np.pi * (minute / 60) / 12)\n",
    "    minute_angle = np.pi / 2 - (2 * np.pi * minute / 60)\n",
    "\n",
    "    # Draw hands\n",
    "    ax.plot([0, 0.5 * np.cos(hour_angle)], [0, 0.5 * np.sin(hour_angle)], 'k', lw=5)  # Hour hand\n",
    "    ax.plot([0, 0.8 * np.cos(minute_angle)], [0, 0.8 * np.sin(minute_angle)], 'k', lw=3)  # Minute hand\n",
    "\n",
    "    # Save image with padding\n",
    "    plt.axis('off')\n",
    "    temp_path = \"data/temp.png\"\n",
    "    plt.savefig(temp_path, bbox_inches='tight', pad_inches=0.1)  # Small padding\n",
    "    plt.close()\n",
    "\n",
    "    # Load image and resize\n",
    "    img = Image.open(temp_path).convert(\"L\")  # Convert to grayscale\n",
    "    img = img.resize((img_size, img_size), Image.LANCZOS)  # Resize to power of 2\n",
    "    img.save(save_path)\n",
    "\n",
    "# Open CSV file for writing labels\n",
    "with open(LABELS_FILE, mode='w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"filename\", \"hour\", \"minute\", \"time_in_minutes\"])\n",
    "\n",
    "    # Generate clock images at different times\n",
    "    for hour in range(12):  # 12-hour format\n",
    "        for minute in range(0, 60, 1):  # Every 5 minutes\n",
    "            filename = f\"clock_{hour:02d}_{minute:02d}.png\"\n",
    "            save_path = os.path.join(IMG_DIR, filename)\n",
    "            draw_clock(hour, minute, save_path)\n",
    "\n",
    "            # Compute single number label (time in minutes past midnight)\n",
    "            time_in_minutes = hour * 60 + minute\n",
    "\n",
    "            # normalize minute, hour, and total minutes\n",
    "            hour_label = hour / 12\n",
    "            minute_label = minute / 60\n",
    "            time_in_minutes_label = time_in_minutes / (12 * 60)\n",
    "\n",
    "            # Write to CSV\n",
    "            writer.writerow([filename, hour_label, minute_label, time_in_minutes_label])\n",
    "\n",
    "print(f\"Clock dataset saved in '{IMG_DIR}', resized to {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"Labels saved in '{LABELS_FILE}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCACAAIABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APf6KKKKKzZvEOi28zRTatYRyKcMj3KAj6gmoz4o8Pjrrmmf+Bcf+NA8UeHz01zTP/AuP/GpIfEOi3EyxQ6tYSSMcKiXKEn6AGtKiiiiiiiikZlRSzEKoGSScACvLfEvxosrfUDo3hGwl8QauTtAhBMKn6jlse3HvWSvgP4j+N/33i3xKdJtH/5cLA849CFIH5lq6HSvgZ4H00KZrGa/lA5e7mJyf91cD9K6O3+Hvg62GI/C+kf8Ds0f+YNFx8PfB1yMSeF9I/4BZon8gK5zVfgZ4H1IMYbGawlI4e0mIwf91sj9K55vAfxH8EfvvCXiU6taJ/y4X55x6AMSPyK1reGvjRZXGoDRvF1hL4f1cHaRMCIWP1PK59+PevUlZXUMpBUjIIPBFLRRRRVXUtSs9I06fUNQuEt7WBS8krnAUf57d68UmvvE/wAbdQktdNabR/BsT7JZyMPc47e5/wBnoO+TgV6v4W8HaJ4O04Wej2axA/6yZuZJT6s3U/ToOwreornvHk81r4A8QT28skM0enzMkkbFWUhDggjkGsv4SXlzf/C7RLm8uJri4kSXfLM5d2xK45J5PArtaKwfFPg7RPGOnGz1iySUAfu5l4kiPqrdR9Oh7ivKIb7xP8EtQjtdSabWPBsr7IpwMva57ex/2eh7YORXtem6lZ6vp0GoafcJcWs6B45UOQw/z27VaoopGZUUsxAUDJJOABXhd7NefG3xq2m2sssPgzSZczyocfan9vc9vQZPUgV7ZYWFppdhBY2NvHb2sCBI4oxhVA7VZoormviH/wAk48Sf9g2f/wBANZPwZ/5JLoP+5L/6Oeu7ooqtf2FpqlhPY31vHcWs6FJIpBlWBrxOymvPgl41XTbqWWbwZq0uYJXOfsr+/uO/qMHqCK90VldQykFSMgg5BFLRXlfxn8SXcOnWXhDRiTq+uOIsKeViJwfpuPH0DV23g7wvZ+D/AAzaaPZgERLmWTHMsh+8x+p/IACt6iiiua+If/JOPEn/AGDZ/wD0A1k/Bn/kkug/7kv/AKOeu7ooorB8Y+F7Pxh4Zu9HvAAJVzFJjmKQfdYfQ/mCRXE/BjxJdzade+ENZJGr6G5iwx5aIHA+u08Z9CteqUV4v4CX/hNvjD4h8XzfPaaYfsVj3A6rkf8AAQx/4HXtFFFFFc18Q/8AknHiT/sGz/8AoBrJ+DP/ACSXQf8Acl/9HPXd0UUUV4v49X/hCfjD4e8Xw/JaamfsV92B6Lk/8BKn/gFe0VmeI5pLfwzqs8LFJY7OZ0YdiEJBri/gdpI0z4YWMpUCS9kkuXPc5O1f/HVFej0UUUVzXxD/AOSceJP+wbP/AOgGsn4M/wDJJdB/3Jf/AEc9d3RRRRXnHxx0kan8ML6UKDLZSR3KHuMHa3/jrGu08OTSXHhnSp5mLyyWcLux7koCTUXis48H60fSwn/9FtVTwBbi2+Hvh2If9A6BvzQH+tdHRRRRXKfEy6itPhn4ikmYqrWMkYIGfmcbFH5sKw/gdqlpffDHT7W3kLTWLSQzqVI2sXZwMnr8rDpXo9FFFFc54/txdfD3xFEf+gdO35IT/SrfhQ58H6KfWwg/9FrR4rGfB+tD1sJ//RbVU8AXAufh74dlH/QOgX8kA/pXR0UUUVwvxk/5JPr3/XOP/wBGpXI/s3/8ifq3/YQ/9prXtFFFFFc54/uBa/D3xFKf+gdOv5oR/WrfhQY8H6KPSwg/9FrUviOGS48M6rBCpeWSzmRFHUkoQBXF/A7VhqfwwsYiwMllJJbOO4wdy/8AjrCvR6KKKK4X4yf8kn17/rnH/wCjUrkf2b/+RP1b/sIf+01r2iiiiivOPjjqw0z4YX0QYCW9kjtkHc5O5v8Ax1TXaeHIZLfwzpUEylJY7OFHU9iEAIrTrxfwE3/CE/GHxD4Qm+S01M/bbHsD1bA/4CWH/AK9ooooorhfjJ/ySfXv+ucf/o1K5H9m/wD5E/Vv+wh/7TWvaKKKKK8X8et/wm3xh8PeEIfntNMP22+7gdGwf+AhR/wOvaKK8r+M/hu7m06y8X6MCNX0NxLlRy0QOT9dp5+hau28HeKLPxh4ZtNYsyAJVxLHnmKQfeU/Q/mCDW9RRRXD/GCKSb4V67HEjO7Rx4VFJJ/ep2Fcp+ztbT23hLVVnhkiY3+QJEKkjy19a9jooorB8Y+KLPwd4Zu9YvCCIlxFHnmWQ/dUfU/kATXE/Bjw3dw6de+L9ZBOr645lyw5WInI+m4849AteqUUjKrqVYAqRggjIIrwu9hvPgl41bUrWKWbwZq0uJ4kGfsr+3uO3qMjqAa9ssL+01SwgvrG4juLWdA8csZyrA1Zoooooooqtf39ppdhPfX1xHb2sCF5JZDhVArxOyhvPjb41XUrqKWHwZpMuIInGPtT+/ue/oMDqSa90VVRQqgBQMAAYAFLRRVXUtNs9X06fT9Qt0uLWdCkkTjIYf579q8UmsfE/wAEtQkutNWbWPBsr75YCcva57+x/wBroe+Dg16v4W8Y6J4x04Xmj3qSgD95C3EkR9GXqPr0PY1vUUUUUVg+KfGOh+DtON5rF6kWR+7hX5pJT6KvU/XoO5ryiGx8T/G3UI7rUlm0fwbE++KAHD3WO/uf9roO2Tk17Xpum2ekadBp+n26W9rAgSOJBgKP89+9WqKKKKRlV1KsAykYIIyCK8t8S/BeyuNQOs+Eb+Xw/q4O4GEkQsfoOV/Dj2rJXx58R/BP7nxb4aOr2if8v9gOcepKgj8wtdDpXxz8D6kFE19Np8pHKXcJGD/vLkfrXR2/xC8HXIzH4o0j/gd4ifzIouPiF4OthmTxRpH/AAC8R/5E1zmq/HPwPpoYQ302oSgcJaQk5P8AvNgfrXPN47+I/jf9z4S8NHSLR/8Al/vxzj1BYAfkGrW8NfBeyt9QGs+Lb+XxBq5O4mYkwqfoeW/Hj2r1JVVFCqAFAwABwBS0UUUUUUUVmzeHtGuJmlm0mwkkY5Z3tkJP1JFRnwv4fPXQ9MP/AG6R/wCFA8L+Hx00PTB/26R/4VJD4f0a3mWWHSbCORTlXS2QEfQgVpUUUUUV/9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAAMBUlEQVR4AWL8zzCwgGlgrWcYdcDAhwALqWng/79/eLQwMTHikcUmRVIu+P+fkYGQBf8Z/jMSUoPsEBIc8O8fKLge7zz5hZGRieHf33//oIHBxMTExMzE8O//fx5zd1kGBoY/TMRnLqIdALL+y4MDay69R7ifnY2JieHfv18/EUKCeiEOCjwkOIE4B/z/z8TAcHfThkMQm7hUVZTlREVFxQRYGRn+//7w6vXr14/u3rn9DSJtF+CnzMDwj7iYIMoBf1gYGI7N2fcQZD6TrqeJnLQUiImGnz19dGb7ZXASlXdKsWJgAGlDU4OF+58g+Pfn/69NNmwgveJ2rVd+gDT8+/Xr9x9wKgClhL9/fv/6BRL+/+Nyi504SCWbzaZf///8AwviJQg64N+///+3+oDMZDBpvQIy69+fP3+xmPzv7x+IhVdaTcDKfbb9/w/SDdKDGxNywJ///08HgHzP7r/57f//WK1GNv3f3z///7/d7M/OwMDAFnD6//8/yLJY2Pgd8O/P/zdlrAyMDNzRl//////7L4YJ/zD9+Pf3////L0dxMzAysJa9IRQPeB3w5///nSag0jr0wP///7GFO4aDwAL/QA49EAIq6E12EQgEfA748/9LCScDI4P17r+4/PHjxo17YDvRiX9//v/ZZcXAyMBZ8gVvNOB2wL+//y+ZMjAyCLT+xuWJf/+vMDAoodsN5f/5//93qwADI4PpZXyBh9MB//793yDJwMTgdAp34P/7f5WBQRVqIQYFiohTTgxMDJIb8eQGXA749/9/BwsDA1vrNzwBiN8B/0EB962VjYGBpeP/fywZF+xkHA74+/9LEgMjg8wWkCFghdgIgg4AOX6zDAMjQ/KX/6CEicUQ7A74+/+TPwMbg8U1fLEH8hW+KADb9u/v/2sWDGwM/p9wuACrA/7+/+LDwMEQ9v4/KEuDDcJKEA4BUOHx/30YAweDD44wwOaAf/8/ejCwMuT+BYUgVpuhgkQ54P+f/39zGVgZPD9iTQdYHPDv//cABhaG4n9/cUQb1HqiogCk9u/ff0UMrAwB37G5ANMB/37/S2JgZyjFk3VApoIwcSEArpJKGTgYkv79xswLmA74+7+DgZkhD3/yA1lPdAj8///v7/9cBmaGTiwJEcMBf/+vY2ZhCIdWrRCLcJHEhsD////+/AljYGFej+kCdAf8/X9RnInB7AO26MJwB/EO+P/v/wdTBiaJixguQHPAv7/v9RiYFW5jqMOwHCRAggP+//1/U4GZQf89ep2K5oC//wsZmDm2Ecp/INv/k5AGQOr//N/KzsxQiO41VAf8+b+LFZRW8Jc/IOPAmJQQ+P//Nyh1s+5C8xyKA/7+faXLwOAEadqBrcBPkOaA//9+OzIw6L1CLV5QHfC/iIGZ7wJ6KOF0BYkO+Pv/Ah8zQzGq8cgO+Pv/NAsTQzeu1g+mO0h0wL/f/7sZmFhOo7gA1QFeDAx239HTKabFMBESHfD/39/vdgwMXrgc8Of/VlZG5sNElQAQJ5DqgP///h9iZmTdipwOESHw7+9XJwaGSNQkArEIF0myA/7//RvBwOD8FSmQEQ74838NMyP/KWTX4bIYJk66A/78P8nHyLwWyRKEA/7/sWJgSCI+BZJaEIFd/e/P/0QGBiukeh7ugL//DzEwct9ASSBgPXgI0kPg/9//N7gZGQ4jrIEPZfxnmM7w30sdIQDuYFKdYPqv7gm2CmYyzAF/me/sZWBI/0/zgdP//9MZGPbcYf4LdQHMAf8ZVr9iMLciblQDqpcsipHRyozh1WoGmE+hDvjP8nEjI0M0519SBrjIc8BfrmgGxo0fWaAugDmA4c65/zJeBAfhyLITVRMjg5f0/3N3YUEAdQDj/wO/GXSV/0K5qFqoy2P6q6LH8Hv/f2hYQ21kZNzAwOhCXZtwm+bMyLABltogDvjP8OAcA5svlIdbJ1VkmBh82RjOPYTGAcQB/xi2fmfQV6VHEmBgYGRQ02f4voXhH9g7EAf8Zzj5n8Eb6iawOC0JsFWnoLbBHPCegcEc6iZa2g02+x+DGQPDOxQHgIaT+GXpEwOgOJDjg/ocZDHYUaCIAY9wQni0JsXV4Z6FRAHIIRqCtK6IYN5i/C+ojmQxDKgz/YExaUwz/mFSh1sBC4G/DNJwMTowZBjQa8N/DCLwaKG5AxgZhOE5DhECdHWACGYIsJHlAFBp9g9asRIfcIwMImwgnSAdoHkoEP2PmywHcDEwkDBBBbIIhBkZRLjR08BfDn6QFKn4BgPDj3OfSdXFwCDAAQsBWBpgYIbWz6QY9o+hkIHpifFRBpIzMCMzzB6YA/4zQstmmARxNBcDAyMzK3FqkVT9R0yWwRyAJEsKExSQf0lOhMg2wBzA+J9+xQCoOoK1yOAOYKDMH8h+Iob9H5YJ4A5g/vGRGI3UUvPhByzo4fTXN2SlQrJc9J/hzVdYNoA74Bd9HfALbjHUC8wM9HUAA0YI0NcBbxFBDw0BFoYnUBZdqCcYIfCf4RZ4gQQ9rP/P8u8m3B5YCPxjuPGeEVSuwWVox/jP+P4mRoPkP8Otl7SzEt3klzfheR4RAh8fwwXR1VOZ/5/h0Se0EGBkEGRgOAl3DZUtRDeOieEUA4MQtO6BhAAjgzkjw1aoELp6qvPBVplBbYM4gInBm5Ph4m36xMF/hlsXGTh94FaDACODghHDr83wiAGJ0Qz/Y9j8i8FIHiUEGP7/D2D4v4dmdqIZvPc/QwBsQBASBQz/GR1YGS7fZaZDSfCP+c4lBlZH2AImqAMYGVSMGJ9so0ci+M+w7SmjkTI0BuAO+MPv/59h6Xdmitp3aCGNlfuf+dtShv8B/H+grXBoCDAwMoSKMZw8BosZrHqpIvj//7FTDGIhsACAO4D5r4oLA8NM2OAZVezCaggj40wGBheVv+jtAQZGhgwGxm03aV0h/WO8uZ2RIRMeAPAQYGBisLL6/7WLgbaR8P8/Q+fX/1ZWyPbCwF/mImbGtadpmxP/MZ9ay8hczARvlSNcwsD0z9P+/8f+f9DUCXMXdWnGf/2f/jt4/IOlfQZQGQibk4FM2x0iYdruz389BiYWht0E1rrALADNduOZtvv//+9/EicuSXUAgYnL/yRP3f75b8TKxsG6h8gQ+Pf7fxfeqdv/f0mcvP7zXx4Ui9uIdMDf/xd4MSavYXUCyCCGfwxvnS8zOO0idnn0f8ba+6yMvyq1kRMV2CSsxP+/bvsZ9PYII6V8lEQIXnxG0gIGROoihgVZwLAbadYUpAs+cQnigCKBlCUc/////vnr10/MxUEQw1BJopZw/IcuYrmFmNlENYV8HpGLWP5DlvGYEreMhwTnQJfxXMLwGVoUgFwAWsgURtRCJuId8O/Pn1DiFjKBXABaypVLzFIuoh1AylKu/+DFbBxELWYj1gH//v0HLWZLJm4x23/wcj5WhiLCy/mIdAB4OR8LQ+APbNUMRhoAr0v46EnUgkbiHABb0PgJm/1YHfCf2CWdRDngN3RJ51eMDADWji0EQAnxUwARi1rBJuAnYItaAz5jtx+HA/7//f+ViGW9+O0Gyf75/x+yrBeH/3E6ABRdhBc2g6zAi//8hyxs7gQlbewqsUcBKCX++7+R0NJu7EbCRClb2g2qFv5fNsW/uB1mFVYatLi9hYLF7eDK+UspaHm/1S6cy/tByrBi8PJ+S9Dy/lJyl/eDXfB/pymo+RByAFRGE1ftQgLv///94A0OppRscAAthv3/FrLFIwrHFg+QO9EweIvHpSgu0BaP8reElmbhTIRQU0GbXALBm1z8SNnk4gfe5BJ4Bu/ifLAVhBwAXl+9DbbNBxQMoDXCSKvhwKaACPg2n8tU3eYDiYdfm2xBocAgbttyGbzR6T+ujU6Xmm3BywDYbInc6ITSKgY1UrFh0J4txFYvDxN5HFu9Hp7ZAdnqpeBI9FYvohzAANvsdhgygALa7CYrKioG3+z26vXrx/DNboy2Af5K1N3sBgKg7X5fH+xfexFpux8bOxMjw/9/P3+BFECwoEGQowI31bf7gQHICQwMj3ed/Aze8Aje8QiWAO93BG945DV3l6HVhkcQGOAtnxAwoJteaQGQhipoYTxhM0cdANiAhwAAuR+v+62m91cAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# Display the first image to verify\n",
    "sample_image_path = os.path.join(IMG_DIR, \"clock_06_00.png\")\n",
    "image = Image.open(sample_image_path)\n",
    "display(image)\n",
    "print(image.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Autoencoder for 1x128x128 inputs\n",
    "class DeepAutoencoder(nn.Module):\n",
    "  def __init__(self, n_hidden):\n",
    "    super(DeepAutoencoder, self).__init__()\n",
    "    \n",
    "    # Encoder: Convolutional layers for feature extraction\n",
    "    self.encoder_conv = nn.Sequential(\n",
    "      nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(2, 2),\n",
    "      nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(2, 2),\n",
    "      nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(2, 2),\n",
    "    )\n",
    "\n",
    "    self.encoder_fc = nn.Sequential(\n",
    "        nn.Linear(512 * 256, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, n_hidden)  # Predicting hour and minute\n",
    "    )\n",
    "\n",
    "    # Decoder: Fully connected layers for reconstruction\n",
    "    self.decoder_fc = nn.Sequential(\n",
    "        nn.Linear(n_hidden, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 512 * 256)\n",
    "    )\n",
    "\n",
    "    # Decoder: Convolutional layers for image reconstruction\n",
    "    self.decoder_conv = nn.Sequential(\n",
    "      nn.ConvTranspose2d(512, 256, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.Upsample(scale_factor=2),\n",
    "      nn.ConvTranspose2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.Upsample(scale_factor=2),\n",
    "      nn.ConvTranspose2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.Upsample(scale_factor=2),\n",
    "      nn.ConvTranspose2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.ConvTranspose2d(32, 1, kernel_size=3, stride=1, padding=1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.encoder_conv(x)\n",
    "    x = torch.flatten(x, start_dim=1)  # Flatten the output\n",
    "    latent = self.encoder_fc(x)  # Encode to latent space\n",
    "\n",
    "    x = self.decoder_fc(latent)  # Decode from latent space\n",
    "    x = x.view(-1, 512, 16, 16)  # Reshape to match decoder input\n",
    "    x = self.decoder_conv(x)  # Decode to image space\n",
    "    reconstructed = torch.sigmoid(x)  # Apply sigmoid to ensure pixel values are between 0 and 1\n",
    "    return reconstructed, latent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Predict (hour, minute) as 2D labels\n",
    "class ClockRegressor2(nn.Module):\n",
    "    def __init__(self, out_dim=2):\n",
    "        super(ClockRegressor2, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512 * 256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, out_dim)  # Predicting hour and minute\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc(x)\n",
    "        x = torch.sigmoid(x) # Ensure output is between 0 and 1\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Custom dataset class\n",
    "class ClockDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None, supervised=True):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.supervised = supervised\n",
    "        self.images = os.listdir(img_dir)\n",
    "        self.labels_df = pd.read_csv(\"data/clock_labels.csv\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.supervised:\n",
    "            label_2d, label_1d = self.get_label(img_name)\n",
    "            return image, label_2d, label_1d\n",
    "        return image\n",
    "    \n",
    "\n",
    "    def get_label(self, img_name):\n",
    "      row = self.labels_df[self.labels_df[\"filename\"] == img_name].iloc[0]\n",
    "\n",
    "      return (\n",
    "          torch.tensor([row[\"hour\"], row[\"minute\"]], dtype=torch.float32),  # 2D label\n",
    "          torch.tensor(row[\"time_in_minutes\"], dtype=torch.float32)  # 1D label\n",
    "      )\n",
    "\n",
    "\n",
    "\n",
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Load supervised dataset\n",
    "train_dataset = ClockDataset(img_dir=IMG_DIR, transform=transform, supervised=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Load unsupervised dataset for autoencoder\n",
    "unsupervised_dataset = ClockDataset(img_dir=IMG_DIR, transform=transform, supervised=False)\n",
    "unsupervised_loader = DataLoader(unsupervised_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of images shape: torch.Size([32, 1, 128, 128])\n",
      "Batch of 2D labels shape: torch.Size([32, 2])\n",
      "Batch of 1D labels shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for batch, labels_2d, labels_1d in train_loader:\n",
    "    print(f\"Batch of images shape: {batch.shape}\")\n",
    "    print(f\"Batch of 2D labels shape: {labels_2d.shape}\")\n",
    "    print(f\"Batch of 1D labels shape: {labels_1d.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27497759461402893\n",
      "0.2654326558113098\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/projects/experiments/.venv/lib/python3.13/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/experiments/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/experiments/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "HIDDEN_UNITS = 2\n",
    "LEARNING_RATE = 0.0002\n",
    "\n",
    "ae = DeepAutoencoder(n_hidden=HIDDEN_UNITS)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(ae.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training loop for autoencoder\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in unsupervised_loader:\n",
    "        # Forward pass\n",
    "        reconstructed, _ = ae(batch) # Get reconstructed image, ignore latent\n",
    "        loss = criterion(reconstructed, batch)\n",
    "        print(loss.item())\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09335006773471832\n",
      "0.08971729129552841\n",
      "0.07255273312330246\n",
      "0.10962341725826263\n",
      "0.07685185968875885\n",
      "0.07334550470113754\n",
      "0.08128383755683899\n",
      "0.0822688490152359\n",
      "0.08242283761501312\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 17\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/projects/experiments/.venv/lib/python3.13/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/experiments/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/experiments/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "LEARNING_RATE = 0.0002\n",
    "\n",
    "regressor = ClockRegressor2(out_dim=2)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(regressor.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training loop for regressor\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for batch, labels_2d, labels_1d in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = regressor(batch)\n",
    "        loss = criterion(outputs, labels_2d)\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
